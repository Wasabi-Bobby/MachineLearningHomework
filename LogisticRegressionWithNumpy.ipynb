{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegressionWithNumpy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wasabi-Bobby/MachineLearningHomework/blob/master/LogisticRegressionWithNumpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nRcKubmnx-16",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images_original, train_labels_original), (test_images_original, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images_original.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "train_labels = train_labels_original.reshape((60000, 1))\n",
        "\n",
        "test_images = test_images_original.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "997xKU-wabVt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(train_images)):\n",
        "      if train_labels[i] == 0:\n",
        "        print(\"Found digit :\" + str(0) + \" | i = \" + str(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9P24ftQD-DRZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid_double(x):\n",
        "    # Simple implementation of the sigmoid function for double values.\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_prime_double(x):\n",
        "    # Simple implementation of the derivative of of the sigmoid function for double values.\n",
        "    return sigmoid_double(x) * (1 - sigmoid_double(x))\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    # Simple implementation of the sigmoid function for vectors.\n",
        "    return np.vectorize(sigmoid_double)(z)\n",
        "\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # Simple implementation of the derivative of the sigmoid function for vectors.\n",
        "    return np.vectorize(sigmoid_prime_double)(z)\n",
        "  \n",
        "class Classifier():\n",
        "  def __init__(self, digit):\n",
        "    self.train_images_personal = numpy.array([])\n",
        "    self.train_label_personal  = numpy.array([])\n",
        "    for i in range(len(train_images)):\n",
        "      if train_label[i] == digit:\n",
        "        train_images_personal.append(train_images[i])\n",
        "        train_label_personal.append(train_label[i])\n",
        "  \n",
        "class Layer:\n",
        "  \n",
        "  def __init__(self, number_classifiers, output_dim):\n",
        "    self.number_classifiers = 10\n",
        "    self.output_dim = output_dim\n",
        "    self.loss_points = numpy.array([])\n",
        "    \n",
        "    self.classifiers = numpy.array([])\n",
        "    \n",
        "    for i in range(number_classifiers):\n",
        "      classifiers.append(Classifier(i))\n",
        "\n",
        "    # Added a division of sqrt(input) for initializaing weights\n",
        "    # This is known as the Xavier Initialization and has less variance\n",
        "    self.weight = np.random.randn(output_dim, number_classifiers) * 1/math.sqrt(number_classifiers)\n",
        "    self.bias = np.random.randn(output_dim, 1)\n",
        "\n",
        "  def forward_prop():\n",
        "    for classifier in number_classifiers:\n",
        "      z = weight.transpose() * feature_batch + bias\n",
        "      a = sigmoid(z)\n",
        "      \n",
        "  def backward_prop():\n",
        "    update_weights()\n",
        "    update_bias()\n",
        "    \n",
        "  \n",
        "  def update_weights(option):\n",
        "    # Have to make this change all weights, so either for loop or make squared error give a matrix to transpose\n",
        "    \n",
        "    if option == \"MSE\"\n",
        "      weight - rate * squared_error_gradient(predictions, label, z, feature)\n",
        "    elif option == \"BCE\"\n",
        "      weight - rate * binary_cross_entropy_deriv(predictions, label)\n",
        "   \n",
        "  def update_bias():\n",
        "    if option == \"MSE\"\n",
        "      bias - rate * squared_error_gradient(predictions, label, z, feature)\n",
        "    elif option == \"BCE\"\n",
        "      bias - rate * binary_cross_entropy_deriv(predictions, label)\n",
        "  \n",
        "  def squared_error_loss(predictions, label):\n",
        "    return 0.5 * (predictions - label) * (predictions - label)\n",
        "    \n",
        "  def squared_error_loss_deriv(predictions, label):\n",
        "    return predictions - label\n",
        "  \n",
        "  def squared_error_gradient(predictions, label, z, feature):\n",
        "    return (predictions - label) * sigmoid_prime(z) * feature\n",
        "  \n",
        "  def binary_cross_entropy(predictions, label):\n",
        "    return -1 * label * math.log(predictions) - (1 - y) * math.log(1-a)\n",
        "  \n",
        "  def binary_cross_entropy_deriv(predictions, label):\n",
        "    return -1 * label * (1 - predictions) + (1 - label) * predictions\n",
        "  \n",
        " \n",
        "\n",
        "'''\n",
        "Make 10 trainers\n",
        "Each handle a digit from 0 - 9\n",
        "Each will train on 9 digits\n",
        "For testing we make all of them test on the number and argmax the highest option\n",
        "If the highest option is 50 percent or below then it will say the the item isn't a number\n",
        "'''\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}